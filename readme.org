#+TITLE: readme
#+DATE: <2018-08-30 Thu>
#+AUTHOR: Jiaxuan Wang
#+EMAIL: jiaxuan@umich
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:nil
#+OPTIONS: todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

use openbox and EYE to correct NN bias

* model archetectures tried

** 0: MLP([d, 8, n_output])  result in models0

#+BEGIN_EXAMPLE
          method name  alpha       auc        ap  min(rec, prec)  sparsity
6            wlasso3*   0.01  0.817193  0.629766        0.449275  0.494893
3            wridge3*    0.1  0.813998  0.775426        0.457143  0.121603
1           eye_loss*   0.01  0.809619  0.645645        0.424460  0.315359
1               r4rr*    0.1  0.789712  0.516031        0.384058  0.062763
2               enet*    0.1  0.778658  0.175591        0.405797  0.632015
0    random_risk_eye*   0.01  0.770915  0.538056        0.370861  0.294015
5              lasso*   0.01  0.766678  0.142867        0.359155  0.105053
4  expert_only_ridge*  1e-05  0.722023  1.000000        0.324324  0.019875
#+END_EXAMPLE

Hypothesis: In each polytope, the correlation structure is different, so that
wlasso doesn't need to trade off between known variables? wridge is again not
sparse as expected, but has a high enough AP due to a very high AUC selected,
*this essentially means that with a lot of regions, wridge can conform to prior
very closely and maintain auc by moving a few really important features up for
each region*.

Note that for wlasso, smaller bias (set w=2 or w=1.5) doesn't have high AP
#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
4   wlasso1_5    0.01  0.792525  0.218658        0.466667  0.170830
2   wlasso1_5   1e-05  0.789593  0.155333        0.414414  0.047898
3   wlasso1_5  0.0001  0.789176  0.120072        0.403846  0.044478
1   wlasso1_5   0.001  0.787851  0.144042        0.439252  0.056265
0   wlasso1_5     0.1  0.630254  0.468528        0.202740  0.900031

  method name   alpha       auc        ap  min(rec, prec)  sparsity
4      wlasso  0.0001  0.806993  0.145987        0.419048  0.047094
0      wlasso   1e-05  0.778072  0.158706        0.410714  0.042646
1      wlasso   0.001  0.775110  0.169379        0.409524  0.055125
3      wlasso    0.01  0.773726  0.302276        0.411215  0.282341
2      wlasso     0.1  0.600917  0.406152        0.200717  0.976824
#+END_EXAMPLE

Furthermore, note that AP is just an approximation of credibility, to really
measure the effect, we still need synthetic data (multiple convex groups, with
each group sharing a line) and symKL.

regular Ridge (on weight not on explanation) result
#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
2       ridge    0.01  0.808660  0.182826        0.386792  0.055379
3       ridge     0.1  0.806889  0.171857        0.449541  0.064763
1       ridge   0.001  0.795532  0.156062        0.405660  0.042789
5      ridge*     0.1  0.787106  0.174303        0.408451  0.061305
4       ridge   1e-05  0.781287  0.137284        0.413462  0.044743
0       ridge  0.0001  0.774291  0.162868        0.394231  0.044774
#+END_EXAMPLE

** 1: MLP([d, n_output]) # should be the same as LR

#+BEGIN_EXAMPLE
          method name  alpha       auc        ap  min(rec, prec)  sparsity
6               enet*   0.01  0.821766  0.142021        0.432624  0.641221
4              lasso*   0.01  0.815823  0.133801        0.434783  0.755725
3             wlasso*   0.01  0.809392  0.264835        0.421429  0.832061
1           eye_loss*   0.01  0.807273  0.589520        0.428571  0.832061
2            wridge3*    0.1  0.805710  0.654924        0.455172  0.083969
5              ridge*  1e-05  0.795050  0.115704        0.414286  0.091603
0    random_risk_eye*   0.01  0.785282  0.587215        0.422535  0.816794
7  expert_only_ridge*    0.1  0.731623  1.000000        0.335714  0.000000
#+END_EXAMPLE

Suprisingly wridge3 has a very high AP (which is understandable to a certain
extent). Upon further inspection, it's not that surprising (notice the low auc
for the wridge with high ap)

#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
3     wridge3  0.0001  0.844904  0.123581        0.461538  0.045802
0     wridge3    0.01  0.844814  0.389861        0.481481  0.068702
4     wridge3   0.001  0.843638  0.148411        0.452830  0.045802
1     wridge3   1e-05  0.842701  0.180381        0.451923  0.045802
2     wridge3     0.1  0.805698  0.582893        0.471154  0.114504
#+END_EXAMPLE

the same thing hold true for wlasso as well

#+BEGIN_SRC 
  method name   alpha       auc        ap  min(rec, prec)  sparsity
4     wlasso3   0.001  0.856960  0.222527        0.490385  0.503817
2     wlasso3  0.0001  0.844978  0.127257        0.457143  0.091603
0     wlasso3   1e-05  0.840587  0.104254        0.443396  0.053435
1     wlasso3    0.01  0.797080  0.517899        0.423423  0.877863
3     wlasso3     0.1  0.549759  0.142108        0.221239  0.022901
#+END_SRC

This time wlasso3 is not selected due to its way too low performance.
Also to be noted is that for wlasso and wridge with w=2 or 1.5 doesn't exibit
the above property: no one with high AP, that signals that they need high bias
to get good result.

Using  original github code for credible learning

#+BEGIN_EXAMPLE
          method name   alpha       auc        ap  min(rec, prec)  sparsity
6              lasso*    0.01  0.819496  0.135070        0.442029  0.748092
5             wlasso*    0.01  0.812830  0.325214        0.437500  0.816794
2           eye_loss*    0.01  0.805726  0.645887        0.428571  0.801527
4            wridge3*     0.1  0.805710  0.654617        0.455172  0.083969
0                owl*  0.0001  0.795227  0.159654        0.413043  0.045802
1    random_risk_eye*    0.01  0.787157  0.622977        0.427586  0.793893
3               enet*     0.1  0.770865  0.121719        0.379310  0.893130
7  expert_only_ridge*     0.1  0.751858  1.000000        0.381295  0.000000
#+END_EXAMPLE

Note that in this run, wridge3 is also selected.

Just to be complete, we tried the same using Right For the Right Reason Penalty

#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
0        r4rr    0.01  0.847285  0.233464        0.490385  0.083969
4        r4rr   1e-05  0.842239  0.113051        0.461538  0.061069
5        r4rr   0.001  0.841599  0.108267        0.451923  0.038168
1        r4rr  0.0001  0.839426  0.119637        0.451923  0.091603
2        r4rr     0.1  0.831731  0.492209        0.490385  0.061069
3       r4rr*  0.0001  0.793461  0.190609        0.434783  0.076336
#+END_EXAMPLE

Note that sparsity is again a concern for r4rr b/c it uses l2 on unknown
variables.

To verify that EYE in the linear case is indeed better than wlasso and wridge, I
performed an experiment using duplicated features for both known and unknown
variables.

#+BEGIN_EXAMPLE
  method name  alpha       auc        ap  min(recall, precision)  sparsity
3    wlasso3*  0.001  0.819320  0.256559                0.434783  0.576336
2      lasso*   0.01  0.817538  0.140888                0.434783  0.748092
1   eye_loss*   0.01  0.812284  0.688439                0.434783  0.793893
4    wridge3*    0.1  0.809114  0.542898                0.445205  0.068702
0       enet*    0.1  0.769907  0.141020                0.382979  0.858779
#+END_EXAMPLE

Indeed wridge AP decreases due to its denseness for unknown feature. Quite
conterintuitively, we don't observe the same pattern for nonlinear classifiers.

To verify advantage over two stage approach, we got the following result
#+BEGIN_EXAMPLE
             method name   alpha       auc        ap  min(recall, precision)  sparsity
13              wlasso3*   0.001  0.819320  0.256559                0.434783  0.576336
3                 lasso*    0.01  0.817538  0.140888                0.434783  0.748092
2              eye_loss*    0.01  0.812284  0.688439                0.434783  0.793893
14              wridge3*     0.1  0.809114  0.542898                0.445205  0.068702
10  two_stage_ridge_0.7*     0.1  0.799153  0.309312                0.422819  0.071429
12  two_stage_ridge_0.9*   0.001  0.798430  0.192545                0.428571  0.071429
11  two_stage_ridge_0.8*  0.0001  0.788527  0.260471                0.413333  0.120482
9   two_stage_ridge_0.6*  0.0001  0.782130  0.365384                0.401361  0.065574
8   two_stage_ridge_0.5*   1e-05  0.780541  0.438777                0.424460  0.109091
0                  enet*     0.1  0.769907  0.141020                0.382979  0.858779
7   two_stage_ridge_0.4*   0.001  0.759373  0.541020                0.397163  0.104167
6   two_stage_ridge_0.3*  0.0001  0.758978  0.504327                0.384058  0.130435
1     expert_only_ridge*     0.1  0.751908  1.000000                0.372414  0.000000
5   two_stage_ridge_0.2*   0.001  0.747991  0.663174                0.366906  0.081081
4   two_stage_ridge_0.1*     0.1  0.730564  0.825059                0.359712  0.033333
#+END_EXAMPLE

Ignore the sparsity for two stage for a moment (the sparsity is calculated based
on their own features not features, that's why they are so low), two stage
approach with the same AP level as eye, performs much worse.

** 2: MLP([d, 30, 10, n_output])

#+BEGIN_EXAMPLE
 method name alpha       auc        ap  min(rec, prec)  sparsity
2            wridge3*   0.1  0.829332  0.718033        0.479452  0.102649
4            wlasso3*  0.01  0.827222  0.672437        0.475524  0.319832
1               enet*   0.1  0.816596  0.190622        0.427536  0.402618
5              lasso*   0.1  0.787392  0.160024        0.366906  0.536046
7              ridge*  0.01  0.779650  0.161136        0.401274  0.047824
0           eye_loss*   0.1  0.775514  0.982717        0.400000  0.855344
6  expert_only_ridge*   0.1  0.758255  1.000000        0.373563  0.016500
3    random_risk_eye*   0.1  0.745721  0.973519        0.363636  0.843366
#+END_EXAMPLE   

Currently overfit very much, try early stopping next run

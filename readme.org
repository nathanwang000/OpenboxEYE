#+TITLE: readme
#+DATE: <2018-08-30 Thu>
#+AUTHOR: Jiaxuan Wang
#+EMAIL: jiaxuan@umich
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:nil
#+OPTIONS: todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

use openbox and EYE to correct NN bias

model archetectures tried

0: MLP([d, 8, n_output])  result in models0

#+BEGIN_EXAMPLE
          method name  alpha       auc        ap  min(rec, prec)  sparsity
6            wlasso3*   0.01  0.817193  0.629766        0.449275  0.494893
3            wridge3*    0.1  0.813998  0.775426        0.457143  0.121603
1           eye_loss*   0.01  0.809619  0.645645        0.424460  0.315359
2               enet*    0.1  0.778658  0.175591        0.405797  0.632015
0    random_risk_eye*   0.01  0.770915  0.538056        0.370861  0.294015
5              lasso*   0.01  0.766678  0.142867        0.359155  0.105053
4  expert_only_ridge*  1e-05  0.722023  1.000000        0.324324  0.019875
#+END_EXAMPLE

Hypothesis: In each polytope, the correlation structure is different, so that
wlasso doesn't need to trade off between known variables? wridge is again not
sparse as expected, but has a high enough AP due to a very high AUC selected,
*this essentially means that with a lot of regions, wridge can conform to prior
very closely and maintain auc by moving a few really important features up for
each region*.

Note that for wlasso, smaller bias (set w=2 or w=1.5) doesn't have high AP
#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
4   wlasso1_5    0.01  0.792525  0.218658        0.466667  0.170830
2   wlasso1_5   1e-05  0.789593  0.155333        0.414414  0.047898
3   wlasso1_5  0.0001  0.789176  0.120072        0.403846  0.044478
1   wlasso1_5   0.001  0.787851  0.144042        0.439252  0.056265
0   wlasso1_5     0.1  0.630254  0.468528        0.202740  0.900031

  method name   alpha       auc        ap  min(rec, prec)  sparsity
4      wlasso  0.0001  0.806993  0.145987        0.419048  0.047094
0      wlasso   1e-05  0.778072  0.158706        0.410714  0.042646
1      wlasso   0.001  0.775110  0.169379        0.409524  0.055125
3      wlasso    0.01  0.773726  0.302276        0.411215  0.282341
2      wlasso     0.1  0.600917  0.406152        0.200717  0.976824
#+END_EXAMPLE

Furthermore, note that AP is just an approximation of credibility, to really
measure the effect, we still need synthetic data (multiple convex groups, with
each group sharing a line) and symKL.

regular Ridge (on weight not on explanation) result
#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
2       ridge    0.01  0.808660  0.182826        0.386792  0.055379
3       ridge     0.1  0.806889  0.171857        0.449541  0.064763
1       ridge   0.001  0.795532  0.156062        0.405660  0.042789
5      ridge*     0.1  0.787106  0.174303        0.408451  0.061305
4       ridge   1e-05  0.781287  0.137284        0.413462  0.044743
0       ridge  0.0001  0.774291  0.162868        0.394231  0.044774
#+END_EXAMPLE

1: MLP([d, n_output]) # should be the same as LR

#+BEGIN_EXAMPLE
          method name  alpha       auc        ap  min(rec, prec)  sparsity
6               enet*   0.01  0.821766  0.142021        0.432624  0.641221
4              lasso*   0.01  0.815823  0.133801        0.434783  0.755725
3             wlasso*   0.01  0.809392  0.264835        0.421429  0.832061
1           eye_loss*   0.01  0.807273  0.589520        0.428571  0.832061
2            wridge3*    0.1  0.805710  0.654924        0.455172  0.083969
5              ridge*  1e-05  0.795050  0.115704        0.414286  0.091603
0    random_risk_eye*   0.01  0.785282  0.587215        0.422535  0.816794
7  expert_only_ridge*    0.1  0.731623  1.000000        0.335714  0.000000
#+END_EXAMPLE

Suprisingly wridge3 has a very high AP (which is understandable to a certain
extent). Upon further inspection, it's not that surprising (notice the low auc
for the wridge with high ap)

#+BEGIN_EXAMPLE
  method name   alpha       auc        ap  min(rec, prec)  sparsity
3     wridge3  0.0001  0.844904  0.123581        0.461538  0.045802
0     wridge3    0.01  0.844814  0.389861        0.481481  0.068702
4     wridge3   0.001  0.843638  0.148411        0.452830  0.045802
1     wridge3   1e-05  0.842701  0.180381        0.451923  0.045802
2     wridge3     0.1  0.805698  0.582893        0.471154  0.114504
#+END_EXAMPLE

the same thing hold true for wlasso as well

#+BEGIN_SRC 
  method name   alpha       auc        ap  min(rec, prec)  sparsity
4     wlasso3   0.001  0.856960  0.222527        0.490385  0.503817
2     wlasso3  0.0001  0.844978  0.127257        0.457143  0.091603
0     wlasso3   1e-05  0.840587  0.104254        0.443396  0.053435
1     wlasso3    0.01  0.797080  0.517899        0.423423  0.877863
3     wlasso3     0.1  0.549759  0.142108        0.221239  0.022901
#+END_SRC

This time wlasso3 is not selected due to its way too low performance.
Also to be noted is that for wlasso and wridge with w=2 or 1.5 doesn't exibit
the above property: no one with high AP, that signals that they need high bias
to get good result.

Using  original github code for credible learning

#+BEGIN_EXAMPLE
          method name   alpha       auc        ap  min(rec, prec)  sparsity
6              lasso*    0.01  0.819496  0.135070        0.442029  0.748092
5             wlasso*    0.01  0.812830  0.325214        0.437500  0.816794
2           eye_loss*    0.01  0.805726  0.645887        0.428571  0.801527
4            wridge3*     0.1  0.805710  0.654617        0.455172  0.083969
0                owl*  0.0001  0.795227  0.159654        0.413043  0.045802
1    random_risk_eye*    0.01  0.787157  0.622977        0.427586  0.793893
3               enet*     0.1  0.770865  0.121719        0.379310  0.893130
7  expert_only_ridge*     0.1  0.751858  1.000000        0.381295  0.000000
#+END_EXAMPLE

Note that in this run, wridge3 is also selected.

2: MLP([d, 30, 10, n_output])
